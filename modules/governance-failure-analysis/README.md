# Governance Failure Analysis Module

This module provides real-world case studies where AI systems caused harm due to missing or weak governance controls.  
Each case study is analysed using the **Big Map Roar Framework**, which traces how harm emerges across the AI lifecycle.

---
## The Big Map Roar Framework (Governance Lens)

** Most AI failures are not model failures — they are governance failures that allowed the system to operate without accountability**.

This framework highlights where systemic harm originates across 5 layers:

1. **Data Governance**  
   If data is unrepresentative → the system learns harm

2. **Model Governance**  
   If model logic is not explainable → harm becomes unchallengeable

3. **Deployment Governance**  
   If human oversight is missing → harm becomes automatic

4. **Monitoring & Incident Response**  
   If behaviour is not observed → harm persists silently

5. **Accountability & Redress (RACI)**  
   If no one owns the outcome → people have **no route to correction**

This framework is used across all case studies to explain:
- What control failed
- How the failure created harm
- How the harm could have been prevented

---
## Case Studies in This Module

| Case | Description | Link |
|------|-------------|------|
| **Deloitte Australia – LLM-Generated Report Failure (2024)** | Incorrect report content due to lack of validation and oversight controls | `./deloitte-llm-hallucination-case/README.md` |

More cases will be added as the repository evolves.

---

## Purpose of This Module

- To **learn from real AI harms**, not theory  
- To **map failures to governance controls**, not model performance  
- To **inform regulatory, audit, and assurance practice**
